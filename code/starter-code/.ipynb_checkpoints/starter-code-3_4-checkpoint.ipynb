{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Lab\n",
    "\n",
    "In this lab we will compare the performance of all the models we have learned about so far, using the car evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n",
    "\n",
    "The [car evaluation dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/car/) is in the assets/datasets folder. By now you should be very familiar with this dataset.\n",
    "\n",
    "1. Load the data into a pandas dataframe\n",
    "- Encode the categorical features properly: define a map that preserves the scale (assigning smaller numbers to words indicating smaller quantities)\n",
    "- Separate features from target into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../../assets/datasets/car.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1728 entries, 0 to 1727\n",
      "Data columns (total 7 columns):\n",
      "buying           1728 non-null object\n",
      "maint            1728 non-null object\n",
      "doors            1728 non-null object\n",
      "persons          1728 non-null object\n",
      "lug_boot         1728 non-null object\n",
      "safety           1728 non-null object\n",
      "acceptability    1728 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 94.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unacc    1210\n",
       "acc       384\n",
       "good       69\n",
       "vgood      65\n",
       "Name: acceptability, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.acceptability.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>acceptability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   buying  maint  doors  persons  lug_boot  safety acceptability\n",
       "0       4      4      1        1         1       1         unacc\n",
       "1       4      4      1        1         1       2         unacc\n",
       "2       4      4      1        1         1       3         unacc\n",
       "3       4      4      1        1         2       1         unacc\n",
       "4       4      4      1        1         2       2         unacc"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_maint = {'vhigh': 4,'high': 3,'med': 2,'low':1}\n",
    "map_doors = {'5more': 4,'4': 3,'3': 2,'2':1}\n",
    "map_persons = {'more': 3,'4': 2,'2':1}\n",
    "map_lug_boot = {'big': 3,'med': 2,'small':1}\n",
    "map_safety = {'high': 3,'med': 2,'low':1}\n",
    "\n",
    "df.maint = df.maint.map(map_maint)\n",
    "df.buying = df.buying.map(map_maint)\n",
    "df.doors = df.doors.map(map_doors)\n",
    "df.persons = df.persons.map(map_persons)\n",
    "df.lug_boot = df.lug_boot.map(map_lug_boot)\n",
    "df.safety = df.safety.map(map_safety)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Useful preparation\n",
    "\n",
    "Since we will compare several models, let's write a couple of helper functions.\n",
    "\n",
    "1. Separate X and y between a train and test set, using 30% test set, random state = 42\n",
    "    - make sure that the data is shuffled and stratified\n",
    "2. Define a function called `evaluate_model`, that trains the model on the train set, tests it on the test, calculates:\n",
    "    - accuracy score\n",
    "    - confusion matrix\n",
    "    - classification report\n",
    "3. Initialize a global dictionary to store the various models for later retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.drop('acceptability', axis=1)\n",
    "y = df.acceptability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, stratify = y, random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, name):\n",
    "    s = cross_val_score(model, X, y, cv=3, n_jobs=-1)\n",
    "    print \"{} Cross Val Score:\\t{:0.3} ± {:0.3}\".format(name, s.mean().round(3), s.std().round(3))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print \"Accuracy: \", model.score(X_test,y_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred, labels =['unacc','acc','good', 'vgood'] ) \n",
    "    print pd.DataFrame(cm, index=['True unacc','True acc','True good', 'True vgood'], \n",
    "                       columns=['Pred unacc','Pred acc','Pred good', 'Pred vgood'] )\n",
    "    print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a KNN\n",
    "\n",
    "Let's start with `KNeighborsClassifier`.\n",
    "\n",
    "1. Initialize a KNN model\n",
    "- Evaluate it's performance with the function you previously defined\n",
    "- Find the optimal value of K using grid search\n",
    "    - Be careful on how you perform the cross validation in the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn Cross Val Score:\t0.739 ± 0.123\n",
      "Accuracy:  0.946050096339\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         355         8          0           0\n",
      "True acc            12       103          0           0\n",
      "True good            0         2         19           0\n",
      "True vgood           0         4          2          14\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.88      0.90      0.89       115\n",
      "       good       0.90      0.90      0.90        21\n",
      "      unacc       0.97      0.98      0.97       363\n",
      "      vgood       1.00      0.70      0.82        20\n",
      "\n",
      "avg / total       0.95      0.95      0.95       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "evaluate_model(knn, 'knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773148148148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 9}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "parameters = {\"n_neighbors\": [1,2,3,4,5,6,7,8,9,10]}\n",
    "\n",
    "gs = GridSearchCV(knn, parameters, cv=5, n_jobs=4)\n",
    "gs.fit(X, y)\n",
    "print gs.best_score_\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn with grid Search Cross Val Score:\t0.775 ± 0.103\n",
      "Accuracy:  0.942196531792\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         357         6          0           0\n",
      "True acc            13       102          0           0\n",
      "True good            2         4         15           0\n",
      "True vgood           1         3          1          15\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.89      0.89      0.89       115\n",
      "       good       0.94      0.71      0.81        21\n",
      "      unacc       0.96      0.98      0.97       363\n",
      "      vgood       1.00      0.75      0.86        20\n",
      "\n",
      "avg / total       0.94      0.94      0.94       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "\n",
    "evaluate_model(knn, 'knn with grid Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Bagging + KNN\n",
    "\n",
    "Now that we have found the optimal K, let's wrap `KNeighborsClassifier` in a BaggingClassifier and see if the score improves.\n",
    "\n",
    "1. Wrap the KNN model in a Bagging Classifier\n",
    "- Evaluate performance\n",
    "- Do a grid search only on the bagging classifier params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn with bagging Cross Val Score:\t0.744 ± 0.129\n",
      "Accuracy:  0.940269749518\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         357         6          0           0\n",
      "True acc            13       100          2           0\n",
      "True good            4         2         15           0\n",
      "True vgood           0         2          2          16\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.91      0.87      0.89       115\n",
      "       good       0.79      0.71      0.75        21\n",
      "      unacc       0.95      0.98      0.97       363\n",
      "      vgood       1.00      0.80      0.89        20\n",
      "\n",
      "avg / total       0.94      0.94      0.94       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "Bknn = BaggingClassifier(knn)\n",
    "\n",
    "evaluate_model(Bknn, 'knn with bagging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.751157407407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'bootstrap_features': False,\n",
       " 'max_features': 5,\n",
       " 'n_estimators': 7}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\"n_estimators\": [1, 3, 5, 7, 9, 11],\n",
    "              'max_features': [1, 2, 3, 4, 5],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"bootstrap_features\": [True, False]}\n",
    "gs = GridSearchCV(Bknn, parameters, cv=5, n_jobs=1)\n",
    "gs.fit(X, y)\n",
    "print gs.best_score_\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn with bagging and bets params Cross Val Score:\t0.704 ± 0.119\n",
      "Accuracy:  0.867052023121\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         356         7          0           0\n",
      "True acc            38        76          1           0\n",
      "True good           12         2          7           0\n",
      "True vgood           5         2          2          11\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.87      0.66      0.75       115\n",
      "       good       0.70      0.33      0.45        21\n",
      "      unacc       0.87      0.98      0.92       363\n",
      "      vgood       1.00      0.55      0.71        20\n",
      "\n",
      "avg / total       0.87      0.87      0.86       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bknn = BaggingClassifier(knn,bootstrap = False,\n",
    " bootstrap_features = False,\n",
    " max_features = 5,\n",
    " n_estimators = 7)\n",
    "\n",
    "evaluate_model(Bknn, 'knn with bagging and bets params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression\n",
    "\n",
    "Let's see if logistic regression performs better\n",
    "\n",
    "1. Initialize LR and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log reg Cross Val Score:\t0.707 ± 0.075\n",
      "Accuracy:  0.782273603083\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         351         9          0           3\n",
      "True acc            64        49          2           0\n",
      "True good            4        10          3           4\n",
      "True vgood           0        17          0           3\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.58      0.43      0.49       115\n",
      "       good       0.60      0.14      0.23        21\n",
      "      unacc       0.84      0.97      0.90       363\n",
      "      vgood       0.30      0.15      0.20        20\n",
      "\n",
      "avg / total       0.75      0.78      0.75       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression()\n",
    "\n",
    "evaluate_model(LR, 'Log reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.754050925926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]}\n",
    "\n",
    "gs = GridSearchCV(LR, parameters, cv=5, n_jobs=1)\n",
    "gs.fit(X, y)\n",
    "print gs.best_score_\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Log reg Cross Val Score:\t0.702 ± 0.096\n",
      "Accuracy:  0.805394990366\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         350        11          1           1\n",
      "True acc            60        53          2           0\n",
      "True good            5        10          6           0\n",
      "True vgood           0        11          0           9\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.62      0.46      0.53       115\n",
      "       good       0.67      0.29      0.40        21\n",
      "      unacc       0.84      0.96      0.90       363\n",
      "      vgood       0.90      0.45      0.60        20\n",
      "\n",
      "avg / total       0.79      0.81      0.79       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(C= 1.0, penalty='l1')\n",
    "BLR = BaggingClassifier(LR)\n",
    "evaluate_model(BLR, 'Bagging Log reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Trees\n",
    "\n",
    "Let's see if Decision Trees perform better\n",
    "\n",
    "1. Initialize DT and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Cross Val Score:\t0.815 ± 0.01\n",
      "Accuracy:  0.982658959538\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         360         3          0           0\n",
      "True acc             3       110          2           0\n",
      "True good            0         0         21           0\n",
      "True vgood           0         1          0          19\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.96      0.96      0.96       115\n",
      "       good       0.91      1.00      0.95        21\n",
      "      unacc       0.99      0.99      0.99       363\n",
      "      vgood       1.00      0.95      0.97        20\n",
      "\n",
      "avg / total       0.98      0.98      0.98       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier()\n",
    "evaluate_model(DT, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857638888889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 9,\n",
       " 'max_features': 4,\n",
       " 'max_leaf_nodes': None}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': [None, 1, 2, 3,4,5],\n",
    "    'max_depth': [None, 4, 5, 6, 7, 8, 9],\n",
    "    'max_leaf_nodes': [None, 4, 5, 6, 7, 8, 9]\n",
    "}\n",
    "gs = GridSearchCV(DT, parameters, cv=5, n_jobs=1)\n",
    "gs.fit(X, y)\n",
    "print gs.best_score_\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Dec Tree Cross Val Score:\t0.818 ± 0.013\n",
      "Accuracy:  0.982658959538\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         360         3          0           0\n",
      "True acc             3       110          2           0\n",
      "True good            0         0         21           0\n",
      "True vgood           0         1          0          19\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.96      0.96      0.96       115\n",
      "       good       0.91      1.00      0.95        21\n",
      "      unacc       0.99      0.99      0.99       363\n",
      "      vgood       1.00      0.95      0.97        20\n",
      "\n",
      "avg / total       0.98      0.98      0.98       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DT = DecisionTreeClassifier()\n",
    "BDT = BaggingClassifier(DT)\n",
    "evaluate_model(DT, 'Bagging Dec Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Support Vector Machines\n",
    "\n",
    "Let's see if SVM perform better\n",
    "\n",
    "1. Initialize SVM and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vector machine Cross Val Score:\t0.764 ± 0.113\n",
      "Accuracy:  0.957610789981\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         352        11          0           0\n",
      "True acc             7       108          0           0\n",
      "True good            0         2         19           0\n",
      "True vgood           0         1          1          18\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.89      0.94      0.91       115\n",
      "       good       0.95      0.90      0.93        21\n",
      "      unacc       0.98      0.97      0.98       363\n",
      "      vgood       1.00      0.90      0.95        20\n",
      "\n",
      "avg / total       0.96      0.96      0.96       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "sv = SVC()\n",
    "\n",
    "evaluate_model(sv, \"Support vector machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/Shreyas/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Support vector machine Cross Val Score:\t0.766 ± 0.109\n",
      "Accuracy:  0.959537572254\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         353        10          0           0\n",
      "True acc             6       109          0           0\n",
      "True good            0         4         17           0\n",
      "True vgood           0         0          1          19\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.89      0.95      0.92       115\n",
      "       good       0.94      0.81      0.87        21\n",
      "      unacc       0.98      0.97      0.98       363\n",
      "      vgood       1.00      0.95      0.97        20\n",
      "\n",
      "avg / total       0.96      0.96      0.96       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bsv = BaggingClassifier(sv)\n",
    "evaluate_model(bsv, \"Bagging Support vector machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.707175925926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "gs = GridSearchCV(sv, parameters, n_jobs=2)\n",
    "gs.fit(X, y)\n",
    "print gs.best_score_\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95568400770712914"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = SVC(gamma=0.001, C=100000.0, kernel='rbf', degree=5)\n",
    "\n",
    "bsv = BaggingClassifier(sv)\n",
    "bsv.fit(X_train,y_train)\n",
    "\n",
    "bsv.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Random Forest & Extra Trees\n",
    "\n",
    "Let's see if Random Forest and Extra Trees perform better\n",
    "\n",
    "1. Initialize RF and ET and test on Train/Test set\n",
    "- Find optimal params with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Val Score:\t0.808 ± 0.07\n",
      "Accuracy:  0.967244701349\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         358         5          0           0\n",
      "True acc             2       111          2           0\n",
      "True good            0         1         20           0\n",
      "True vgood           0         5          2          13\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.91      0.97      0.94       115\n",
      "       good       0.83      0.95      0.89        21\n",
      "      unacc       0.99      0.99      0.99       363\n",
      "      vgood       1.00      0.65      0.79        20\n",
      "\n",
      "avg / total       0.97      0.97      0.97       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "evaluate_model(RFC, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.883101851852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 11, 'max_features': 5}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = {\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_features': [1, 2, 3, 4,5,6],\n",
    "    'max_depth': [None, 3, 5, 7, 9, 11,13, 15,17]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(RFC, param,cv=5, n_jobs=2)\n",
    "gs.fit(X, y)\n",
    "print gs.best_score_\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Val Score:\t0.854 ± 0.045\n",
      "Accuracy:  0.97880539499\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         358         5          0           0\n",
      "True acc             3       111          1           0\n",
      "True good            0         0         21           0\n",
      "True vgood           0         0          2          18\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.96      0.97      0.96       115\n",
      "       good       0.88      1.00      0.93        21\n",
      "      unacc       0.99      0.99      0.99       363\n",
      "      vgood       1.00      0.90      0.95        20\n",
      "\n",
      "avg / total       0.98      0.98      0.98       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(bootstrap = True,\n",
    " criterion= 'entropy',\n",
    " max_depth = 11,\n",
    " max_features = 5)\n",
    "\n",
    "evaluate_model(RFC, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree Classfifier Cross Val Score:\t0.861 ± 0.015\n",
      "Accuracy:  0.959537572254\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         355         8          0           0\n",
      "True acc            10       105          0           0\n",
      "True good            0         3         18           0\n",
      "True vgood           0         0          0          20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.91      0.91      0.91       115\n",
      "       good       1.00      0.86      0.92        21\n",
      "      unacc       0.97      0.98      0.98       363\n",
      "      vgood       1.00      1.00      1.00        20\n",
      "\n",
      "avg / total       0.96      0.96      0.96       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ETC = ExtraTreesClassifier()\n",
    "\n",
    "evaluate_model(ETC, \"Extra Tree Classfifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.867476851852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'max_features': 6}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = {\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_features': [1, 2, 3, 4,5,6],\n",
    "    'max_depth': [None, 3, 5, 7, 9,11,13, 15,17]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(RFC, param,cv=5, n_jobs=2)\n",
    "gs.fit(X, y)\n",
    "print gs.best_score_\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree Classfifier Cross Val Score:\t0.835 ± 0.035\n",
      "Accuracy:  0.976878612717\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         357         6          0           0\n",
      "True acc             3       111          1           0\n",
      "True good            0         0         20           1\n",
      "True vgood           0         1          0          19\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.94      0.97      0.95       115\n",
      "       good       0.95      0.95      0.95        21\n",
      "      unacc       0.99      0.98      0.99       363\n",
      "      vgood       0.95      0.95      0.95        20\n",
      "\n",
      "avg / total       0.98      0.98      0.98       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ETC = ExtraTreesClassifier(bootstrap = True,max_depth= None,\n",
    " criterion= 'entropy',\n",
    " max_features = 6)\n",
    "\n",
    "evaluate_model(ETC, \"Extra Tree Classfifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model comparison\n",
    "\n",
    "Let's compare the scores of the various models.\n",
    "\n",
    "1. Do a bar chart of the scores of the best models. Who's the winner on the train/test split?\n",
    "- Re-test all the models using a 3 fold stratified shuffled cross validation\n",
    "- Do a bar chart with errorbars of the cross validation average scores. is the winner the same?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is RandomForestClassifier with Parameters from GridSearch\n",
      "\n",
      "Random Forest Cross Val Score:\t0.83 ± 0.052\n",
      "Accuracy:  0.976878612717\n",
      "            Pred unacc  Pred acc  Pred good  Pred vgood\n",
      "True unacc         358         5          0           0\n",
      "True acc             3       112          0           0\n",
      "True good            0         1         20           0\n",
      "True vgood           0         2          1          17\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acc       0.93      0.97      0.95       115\n",
      "       good       0.95      0.95      0.95        21\n",
      "      unacc       0.99      0.99      0.99       363\n",
      "      vgood       1.00      0.85      0.92        20\n",
      "\n",
      "avg / total       0.98      0.98      0.98       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"The best model is RandomForestClassifier with Parameters from GridSearch\\n\"\n",
    "evaluate_model(RFC, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "We have encoded the data using a map that preserves the scale.\n",
    "Would our results have changed if we had encoded the categorical data using `pd.get_dummies` or `OneHotEncoder`  to encode them as binary variables instead?\n",
    "\n",
    "1. Repeat the analysis for this scenario. Is it better?\n",
    "- Experiment with other models or other parameters, can you beat your classmates best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
